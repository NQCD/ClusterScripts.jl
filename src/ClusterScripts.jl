module ClusterScripts

# Write your package code here.
using Distributed
using DiffEqBase
using JLD2
using RobustPmap

"""
    loadbalance_run_dynamics(target_function::Function, input_parameters::Dict; trajectories_key="trajectories", ensemble_key="ensemble_algorithm")

    Distribute tasks based on the EnsembleAlgorithm selected for NQCD. 
    `target_function` must contain an instance of NQCDynamics.run_dynamics() and must return the results of run_dynamics and the input_parameters as a tuple: `(results, input_parameters)`
    `trajectories_key` determines the key for the number of trajectories (Default: `trajectories`). Modify for method-specific keys such as `trajectories_fssh`. 
    `ensemble_key` determines the key for the selected EnsembleAlgorithm (Default: `ensemble_algorithm`).
    
"""
function loadbalance_run_dynamics(target_function, input_parameters::Dict; trajectories_key="trajectories", ensemble_key="ensemble_algorithm")
    # Check if trajectories and ensemble key are defined. 
    if !haskey(input_parameters, trajectories_key)
        throw(ArgumentError("Input parameters are missing the trajectories key"))
    end
    if !haskey(input_parameters, ensemble_key)
        throw(ArgumentError("Input parameters are missing the ensemble algorithm key"))
    end
    # Multiprocessing + Multithreading : DiffEqBase handles work distribution
    if input_parameters[ensemble_key]==EnsembleSplitThreads()
        return target_function(input_parameters)
    end
    # Multithreading case. Split function execution into roughly equal chunks and recombine outputs. 
    if input_parameters[ensemble_key]==EnsembleThreads() && length(workers())>1
        # Number of chunks should be number of workers or number of trajectories, if that is lower
        chunks=input_parameters[trajectories_key]<length(workers()) ? input_parameters[trajectories_key] : length(workers())
        # Chunk size should be 1 if less workers than trajectories, otherwise trajectories/workers rounded down to the nearest int. 
        chunk_size=input_parameters[trajectories_key]<length(workers()) ? 1 : Int(floor(input_parameters[trajectories_key]/length(workers())))
        # Modify simulation inputs
        modified_input=input_parameters
        modified_input[trajectories_key]=chunk_size
        input_list=[modified_input for i in 1:chunks]
        # Add any remaining trajectories if necessary
        input_list[end][trajectories_key]+=input_parameters[trajectories_key]<length(workers()) ? 0 : input_parameters[trajectories_key]%length(workers())
        results=pmap(target_function, input_list)
    end
    # Single threading case. Run single trajectories on all available workers. 
    if input_parameters[ensemble_key]==EnsembleSerial() && length(workers())>1
        chunks=input_parameters[trajectories_key]
        chunk_size=1
        modified_input=input_parameters
        modified_input[trajectories_key]=chunk_size
        input_list=[modified_input for i in 1:chunks]
        results=pmap(target_function, input_list)
    end
    if length(workers())==1
        @info "Running loadbalance_queue with only one worker. Is that intentional?"
        results=pmap(target_function, input_list)
    end
    # Now munch the data into the expected output format for run_dynamics with multiple trajectories. 
    final_output=[]
    for result in results
        # Add results to a vector, flatten if already a vector. 
        if isa(result[1], Dict)
            push!(final_output, result[1])
        else
            push!(final_output, result[1]...)
        end
    end
    final_parameters=results[1][2]
    final_parameters[trajectories_key]=input_parameters[trajectories_key]
    return final_output, final_parameters
end

"""
    loadbalance_queue(target_function::Function, input_list::Vector{Dict}; trajectories_key="trajectories", ensemble_key="ensemble_algorithm", tmp_dir="tmp/", checkpoint_frequency=0, sort_variable="")
    Splits a list of input parameters into smaller operations for better multiprocessing, then runs the target function on the list of inputs and merges the results back together again. 

    Arguments similar to pmap: loadbalance_queue(f, args)

    `trajectories_key`: Dictionary key in input parameter dictionary holding the number of trajectories for target_function to perform. 
    
    `ensemble_key`: Dictionary key in input parameters containing the EnsembleAlgorithm for NQCD to use. 

    `tmp_dir`: Location for checkpoint files. 

    `checkpoint_frequency`: Number of operations per worker to perform before saving to a tempfile. (Default is 1 per worker - the faster and more stable, the higher to go)

    `sort_variable`: Offers the option to sort all jobs generated by a certain input parameter to group tasks of similar size together. 
TBW
"""
function loadbalance_queue(target_function, input_list::Vector{Dict}; trajectories_key="trajectories", ensemble_key="ensemble_algorithm", tmp_dir="tmp/", checkpoint_frequency=0, sort_variable="")
    # Make temp dir, if it doesn't already exist
    if !isdir(tmp_dir)
        mkdir(tmp_dir)
    end
    # Give each combination of input parameters a unique ID to allow for easier merging later. 
    for i in 1:length(input_list)
        input_list[i]["jobid"]=i
    end
    # The simulation queue is organised by indices from the input_list to avoid loading many copies of the same input data. 
    simulation_queue=collect(1:length(input_list))
    # Threaded NQCD jobs are split into jobs per worker. 
    multithread_jobs=findall(x-> x[ensemble_key]==EnsembleThreads() && x[trajectories_key]>1, input_list)
    for index in multithread_jobs
        total_trj=input_list[index][trajectories_key]
        # Number of chunks should be number of workers or number of trajectories, if that is lower
        chunks=total_trj<length(workers()) ? total_trj : length(workers())
        # Chunk size should be 1 if less workers than trajectories, otherwise trajectories/workers rounded down to the nearest int. 
        chunk_size=total_trj<length(workers()) ? 1 : Int(floor(total_trj/length(workers())))
        # The total number of trajectories is preserved by splitting into >=2 chunks, whereby one of them takes on the remainder, if there is one. 
        modified_parameters=input_list[index]
        modified_parameters[trajectories_key]=chunk_size+total_trj%chunks
        input_list[index][trajectories_key]=chunk_size
        # Add to the simulation queue
        push!(input_list, modified_parameters)
        push!(simulation_queue, length(input_list))
        for n in 3:Int(chunks)
            push!(simulation_queue, index)
        end
    end
    # Serial and distributed NQCD jobs are split into single trajectory runs for checkpointing. 
    serial_jobs=findall(x-> (x[ensemble_key]==EnsembleSerial() || x[ensemble_key]==EnsembleDistributed()) && x[trajectories_key]>1, input_list)
    for index in serial_jobs
        n_traj=input_list[index][trajectories_key]
        input_list[index][trajectories_key]=1
        for n in 1:n_traj-1
            push!(simulation_queue, index)
        end
    end
    # All operations should now be atomised. Create a view to pmap target function over. 
    simulation_queue=input_list[simulation_queue]
    # Calculations are now split into chunks, which should take roughly the same amount of work to complete, otherwise one slow process will hold up all others, as all work must stop before a checkpoint. 
    # Put chunks of the same job in order. 
    sort!(simulation_queue, by=x->x["jobid"])
    # If a sort_variable is defined, the simulation queue is sorted by it (low to high)
    if sort_variable==""
        # If no sort order is specified, just work on the array in the order it's already in. 
        spx=Colon()
        spx_rev=Colon()
    else
        # Permutations for sorting forward
        spx=sortperm(simulation_queue, by=x->x[sort_variable])
        # Permutations to sort back into original order, so as not to destroy the variable order.  
        spx_rev=sortperm(spx)
    end
    # Now apply the sort order, or stay in the original order. 
    active_simulation_queue=view(simulation_queue, spx)
    @info "Now starting simulation queue:"
    # Target function must provide output as Tuples (output from run_dynamics, input data)
    @time "Simulation queue" results=RobustPmap.crpmap(target_function, checkpoint_frequency==0 ? length(workers()) : length(workers())*checkpoint_frequency, tmp_dir*"tempfile", active_simulation_queue)
    # Since pmap creates a new array, sort it back into original order. 
    unsorted_results=results[spx_rev]
    # Process results, merging all split jobs back together. 
    return merge_results(unsorted_results;trajectories_key=trajectories_key)
end

"""
    merge_results(simulation_output;trajectories_key="trajectories", ensemble_key="ensemble_algorithm")
    Merger function for NQCD simulation results. Takes a vector of (NQCD output, input_parameters) tuples and merges together all unique combinations of simulation parameters, adding the number of trajectories together as if the simulation was run as a larger ensemble. 
TBW
"""
function merge_results(simulation_output;trajectories_key="trajectories", ensemble_key="ensemble_algorithm")
    @info "Now merging simulation results"
    # Function to merge NQCD outputs
    function merge_nqcd_outputs(first_output, other_outputs...)
        for i in other_outputs
            for (k,v) in i[2]
                if k==trajectories_key
                    first_output[2][trajectories_key]+=v
                end
            end
            if !isa(first_output[1], Vector)
                #? Can't modify first_output[1] directly due to data type of the results tuple. 
                first_output=([first_output[1]], first_output[2])
            end
            if isa(i[1], Vector)
                push!(first_output[1], i[1]...)
            else
                push!(first_output[1], i[1])
            end
        end
        return first_output
    end
    # Find all unique combinations of simulation parameters processed
    jobids=unique([simulation_output[i][2]["jobid"] for i in eachindex(simulation_output)])
    for id in jobids
        to_merge=findall(x->x[2]["jobid"]==id, simulation_output)
        if length(to_merge)>1
            simulation_output[to_merge[1]]=merge_nqcd_outputs(simulation_output[to_merge]...)
            # Get rid of the output of all merged results, in reverse to preserve indices. 
            for i in reverse(to_merge[2:end])
                popat!(simulation_output, i)
            end
        end
    end
    # Finally sort the simulation results in case something was misaligned
    sort!(simulation_output, by=x->x[2]["jobid"])
    return simulation_output
end


"""
    build_job_queue(fixed_parameters::Dict, variables::Dict)
    Returns a Vector of all unique combinations of values in `variables` merged with `fixed_parameters`. 
TBW
"""
function build_job_queue(fixed_parameters::Dict, variables::Dict)
    merged_combinations=Vector{Dict}()
    variable_combinations=reshape(collect(Iterators.product(values(variables)...)), :)
    for i in eachindex(variable_combinations)
        push!(merged_combinations, merge(fixed_parameters, Dict([(collect(keys(variables))[j], variable_combinations[i][j]) for j in 1:length(keys(variables))])))
    end
    return merged_combinations
end

"""
    build_job_queue(fixed_parameters::Dict, variables::Dict, postprocessing_function::Function)

    Returns a Vector of all unique combinations of values in `variables` merged with `fixed_parameters`. 
    By specifying a `postprocessing_function`, further actions can be performed each of the elements in the resulting vector. 
"""
function build_job_queue(fixed_parameters::Dict, variables::Dict, postprocessing_function::Function)
    merged_combinations=Vector{Dict}()
    variable_combinations=reshape(collect(Iterators.product(values(variables)...)), :)
    for i in eachindex(variable_combinations)
        push!(merged_combinations, merge(fixed_parameters, Dict([(collect(keys(variables))[j], variable_combinations[i][j]) for j in 1:length(keys(variables))])))
    end
    # Accept a function that does in-place modification of the input parameters dictionary
    return map(postprocessing_function,merged_combinations)
end

export loadbalance_run_dynamics,build_job_queue, merge_results, loadbalance_queue

end
